{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the multilayer perceptron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network as an XOR Gate...\n",
      "\n",
      "0.2754118120217931\n",
      "0.25640119459857064\n",
      "0.24825203849967992\n",
      "0.22900344563001238\n",
      "0.20114977880322105\n",
      "0.17958199560136257\n",
      "0.16415695328503482\n",
      "0.1537633350118473\n",
      "0.14727516477678046\n",
      "0.14315312130961197\n",
      "0.1403946196247723\n",
      "0.13845234215684088\n",
      "0.13702533811690676\n",
      "0.1359400500974187\n",
      "0.13509111907465027\n",
      "0.13441154133907568\n",
      "0.13385695217516058\n",
      "0.1333969435285084\n",
      "0.1330100463336673\n",
      "0.13268071055387398\n",
      "0.13239742093154172\n",
      "0.13215148335372146\n",
      "0.13193622087098655\n",
      "0.1317464276548734\n",
      "0.1315779898224999\n",
      "0.1314276168610628\n",
      "0.13129264797281376\n",
      "0.1311709101784737\n",
      "0.1310606128196562\n",
      "0.13096026807617733\n",
      "\n",
      "Layer 2 Neuron 0 [-7.86722247 -5.14831487  1.11966177]\n",
      "Layer 2 Neuron 1 [-6.56989486  1.26218126 -0.59041099]\n",
      "Layer 3 Neuron 0 [-6.77153673  4.85288022 -0.02919154]\n",
      "\n",
      "MLP:\n",
      "0 0 = 0.0321650744\n",
      "0 1 = 0.9554049741\n",
      "1 0 = 0.4916599328\n",
      "1 1 = 0.4960113321\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"A single neuron with the sigmoid activation function.\n",
    "       Attributes:\n",
    "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
    "          bias:   The bias term. By default it's 1.0.\"\"\"\n",
    "\n",
    "    def __init__(self, inputs, bias=1.0):\n",
    "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias).\"\"\"\n",
    "        self.weights = (np.random.rand(inputs+1) * 2) - 1\n",
    "        self.bias = bias\n",
    "\n",
    "    def run(self, x):\n",
    "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
    "        x_sum = np.dot(np.append(x, self.bias), self.weights)\n",
    "        return self.sigmoid(x_sum)\n",
    "\n",
    "    def set_weights(self, w_init):\n",
    "        \"\"\"Set the weights. w_init is a python list with the weights.\"\"\"\n",
    "        self.weights = np.array(w_init)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Evaluate the sigmoid function for the floating point input x.\"\"\"\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    \"\"\"A multilayer perceptron class that uses the Perceptron class above.\n",
    "       Attributes:\n",
    "          layers:  A python list with the number of elements per layer.\n",
    "          bias:    The bias term. The same bias is used for all neurons.\n",
    "          eta:     The learning rate.\"\"\"\n",
    "\n",
    "    def __init__(self, layers, bias=1.0, eta=0.5):\n",
    "        \"\"\"Return a new MLP object with the specified parameters.\"\"\"\n",
    "        self.layers = np.array(layers, dtype=object)\n",
    "        self.bias = bias\n",
    "        self.eta = eta\n",
    "        self.network = []  # The list of lists of neurons\n",
    "        self.values = []  # The list of lists of output values\n",
    "        self.d = []       # The list of lists of error terms (lowercase deltas)\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            self.values.append([])\n",
    "            self.d.append([])\n",
    "            self.network.append([])\n",
    "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
    "            self.d[i] = [0.0 for j in range(self.layers[i])]\n",
    "            if i > 0:  # network[0] is the input layer, so it has no neurons\n",
    "                for j in range(self.layers[i]):\n",
    "                    self.network[i].append(Perceptron(\n",
    "                        inputs=self.layers[i-1], bias=self.bias))\n",
    "\n",
    "        self.network = np.array([np.array(x)\n",
    "                                for x in self.network], dtype=object)\n",
    "        self.values = np.array([np.array(x)\n",
    "                               for x in self.values], dtype=object)\n",
    "        self.d = np.array([np.array(x) for x in self.d], dtype=object)\n",
    "\n",
    "    def set_weights(self, w_init):\n",
    "        \"\"\"Set the weights. \n",
    "           w_init is a 3D list with the weights for all but the input layer.\"\"\"\n",
    "        for i in range(len(w_init)):\n",
    "            for j in range(len(w_init[i])):\n",
    "                self.network[i+1][j].set_weights(w_init[i][j])\n",
    "\n",
    "    def print_weights(self):\n",
    "        print()\n",
    "        for i in range(1, len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                print(\"Layer\", i+1, \"Neuron\", j, self.network[i][j].weights)\n",
    "        print()\n",
    "\n",
    "    def run(self, x):\n",
    "        \"\"\"Feed a sample x into the MultiLayer Perceptron.\"\"\"\n",
    "        x = np.array(x, dtype=object)\n",
    "        self.values[0] = x\n",
    "        for i in range(1, len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                self.values[i][j] = self.network[i][j].run(self.values[i-1])\n",
    "        return self.values[-1]\n",
    "\n",
    "    def bp(self, x, y):\n",
    "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm.\"\"\"\n",
    "        x = np.array(x, dtype=object)\n",
    "        y = np.array(y, dtype=object)\n",
    "\n",
    "        # Backpropagation Step by Step:\n",
    "\n",
    "        # STEP 1: Feed a sample to the network\n",
    "        outputs = self.run(x)\n",
    "\n",
    "        # STEP 2: Calculate the MSE\n",
    "        error = (y - outputs)\n",
    "        MSE = sum(error ** 2) / self.layers[-1]\n",
    "\n",
    "        # STEP 3: Calculate the output error terms\n",
    "        self.d[-1] = outputs * (1 - outputs) * (error)\n",
    "\n",
    "        # STEP 4: Calculate the error term of each unit on each layer\n",
    "        for i in reversed(range(1, len(self.network)-1)):\n",
    "            for h in range(len(self.network[i])):\n",
    "                fwd_error = 0.0\n",
    "                for k in range(self.layers[i+1]):\n",
    "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
    "                self.d[i][h] = self.values[i][h] * (1-self.values[i][h]) * fwd_error\n",
    "\n",
    "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
    "        for i in range(1, len(self.network)):\n",
    "            for j in range(self.layers[i]):\n",
    "                for k in range(self.layers[i-1]+1):\n",
    "                    if k == self.layers[i-1]:\n",
    "                        delta = self.eta * self.d[i][j] * self.bias\n",
    "                    else:\n",
    "                        delta = self.eta * self.d[i][j] * self.values[i-1][k]\n",
    "                    self.network[i][j].weights[k] += delta\n",
    "        return MSE\n",
    "\n",
    "\n",
    "# test code\n",
    "mlp = MultiLayerPerceptron(layers=[2, 2, 1])\n",
    "print(\"\\nTraining Neural Network as an XOR Gate...\\n\")\n",
    "for i in range(3000):\n",
    "    mse = 0.0\n",
    "    mse += mlp.bp([0, 0], [0])\n",
    "    mse += mlp.bp([0, 1], [1])\n",
    "    mse += mlp.bp([1, 0], [1])\n",
    "    mse += mlp.bp([1, 1], [0])\n",
    "    mse = mse / 4\n",
    "    if (i % 100 == 0):\n",
    "        print(mse)\n",
    "\n",
    "mlp.print_weights()\n",
    "\n",
    "print(\"MLP:\")\n",
    "print(\"0 0 = {0:.10f}\".format(mlp.run([0, 0])[0]))\n",
    "print(\"0 1 = {0:.10f}\".format(mlp.run([0, 1])[0]))\n",
    "print(\"1 0 = {0:.10f}\".format(mlp.run([1, 0])[0]))\n",
    "print(\"1 1 = {0:.10f}\".format(mlp.run([1, 1])[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
